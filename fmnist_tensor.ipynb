{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fmnist_tensor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVV4gedk/BjKy3rbGQjAo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hardikr23/tensorflow_codes/blob/main/fmnist_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3NYduY2vMOM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "fmnist_data = keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_test, y_test) = fmnist_data.load_data()"
      ],
      "metadata": {
        "id": "EDARmi7evcOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_train)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dKfcysev2Fl",
        "outputId": "05f3da90-c586-4bcc-b697-6d176b052028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=320)\n",
        "print(x_train[1,:])\n",
        "print(y_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXsiEpydwDCw",
        "outputId": "3f3531b1-3ae9-490c-ffbb-924dda951115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168 133  16   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0  49 136 219 216 228 236 255 255 255 255 217 215 254 231 160  45   0   0   0   0   0]\n",
            " [  0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204 202 201 201 201 209 218 224 164   0   0   0   0]\n",
            " [  0   0   0   0   0 188 219 200 198 202 198 199 199 201 196 198 198 200 200 200 200 201 200 225  41   0   0   0]\n",
            " [  0   0   0   0  51 219 199 203 203 212 238 248 250 245 249 246 247 252 248 235 207 203 203 222 140   0   0   0]\n",
            " [  0   0   0   0 116 226 206 204 207 204 101  75  47  73  48  50  45  51  63 113 222 202 206 220 224   0   0   0]\n",
            " [  0   0   0   0 200 222 209 203 215 200   0  70  98   0 103  59  68  71  49   0 219 206 214 210 250  38   0   0]\n",
            " [  0   0   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255 205   0 215 217 214 208 220  95   0   0]\n",
            " [  0   0   0  45 226 214 214 215 224 205   0  42  35  60  16  17  12  13  70   0 189 216 212 206 212 156   0   0]\n",
            " [  0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76  92  87 206 207 222 213 219 208   0   0]\n",
            " [  0   0   0 106 187 223 237 248 211 198 252 250 248 245 248 252 253 250 252 239 201 212 225 215 193 113   0   0]\n",
            " [  0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200 200 201 203 195 210 165   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  47 225 192 214 203 206 204 204 205 206 204 212 197 218 107   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   6   0  46 212 195 212 202 206 205 204 205 206 204 212 200 218  91   0   3   1   0   0   0]\n",
            " [  0   0   0   0   0   1   0  11 197 199 205 202 205 206 204 205 207 204 205 205 218  77   0   5   0   0   0   0]\n",
            " [  0   0   0   0   0   3   0   2 191 198 201 205 206 205 205 206 209 206 199 209 219  74   0   5   0   0   0   0]\n",
            " [  0   0   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208 198 207 221  72   0   4   0   0   0   0]\n",
            " [  0   0   0   0   0   2   0   0 215 198 203 206 208 205 207 207 210 208 200 202 222  75   0   4   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206 205 198 221  80   0   3   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 204 201 205 208 207 205 211 205 210 210 209 195 221  96   0   3   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206 210 209 210 194 217 105   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 204 204 205 208 207 205 215 207 210 208 211 193 213 115   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 204 207 207 208 206 206 215 210 210 207 212 195 210 118   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 198 208 208 208 204 207 212 212 210 207 211 196 207 121   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 198 210 207 208 206 209 213 212 211 207 210 197 207 124   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201 205 197 206 127   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 188 221 214 234 236 238 244 244 244 240 243 214 224 162   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121 119 114 130  76   0   0   0   0   0   0]]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "id": "v1T0eEbEwSF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7TGslb2xFBr",
        "outputId": "933aec35-3c78-4ba8-c4f1-84289a5526c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.00392157 0.         0.         0.         0.         0.16078431 0.7372549  0.40392157 0.21176471 0.18823529 0.16862745 0.34117647 0.65882353 0.52156863 0.0627451  0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.00392157 0.         0.         0.         0.19215686 0.53333333 0.85882353 0.84705882 0.89411765 0.9254902  1.         1.         1.         1.         0.85098039 0.84313725 0.99607843 0.90588235 0.62745098 0.17647059 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.05490196 0.69019608 0.87058824 0.87843137 0.83137255 0.79607843 0.77647059 0.76862745 0.78431373 0.84313725 0.8        0.79215686 0.78823529 0.78823529 0.78823529 0.81960784 0.85490196 0.87843137 0.64313725 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.7372549  0.85882353 0.78431373 0.77647059 0.79215686 0.77647059 0.78039216 0.78039216 0.78823529 0.76862745 0.77647059 0.77647059 0.78431373 0.78431373 0.78431373 0.78431373 0.78823529 0.78431373 0.88235294 0.16078431 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.2        0.85882353 0.78039216 0.79607843 0.79607843 0.83137255 0.93333333 0.97254902 0.98039216 0.96078431 0.97647059 0.96470588 0.96862745 0.98823529 0.97254902 0.92156863 0.81176471 0.79607843 0.79607843 0.87058824 0.54901961 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.45490196 0.88627451 0.80784314 0.8        0.81176471 0.8        0.39607843 0.29411765 0.18431373 0.28627451 0.18823529 0.19607843 0.17647059 0.2        0.24705882 0.44313725 0.87058824 0.79215686 0.80784314 0.8627451  0.87843137 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.78431373 0.87058824 0.81960784 0.79607843 0.84313725 0.78431373 0.         0.2745098  0.38431373 0.         0.40392157 0.23137255 0.26666667 0.27843137 0.19215686 0.         0.85882353 0.80784314 0.83921569 0.82352941 0.98039216 0.14901961 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.96862745 0.85490196 0.83137255 0.82352941 0.84313725 0.83921569 0.         0.99607843 0.95294118 0.54509804 1.         0.68235294 0.98431373 1.         0.80392157 0.         0.84313725 0.85098039 0.83921569 0.81568627 0.8627451  0.37254902 0.         0.        ]\n",
            " [0.         0.         0.         0.17647059 0.88627451 0.83921569 0.83921569 0.84313725 0.87843137 0.80392157 0.         0.16470588 0.1372549  0.23529412 0.0627451  0.06666667 0.04705882 0.05098039 0.2745098  0.         0.74117647 0.84705882 0.83137255 0.80784314 0.83137255 0.61176471 0.         0.        ]\n",
            " [0.         0.         0.         0.64313725 0.92156863 0.83921569 0.82745098 0.8627451  0.84705882 0.78823529 0.20392157 0.27843137 0.34901961 0.36862745 0.3254902  0.30588235 0.2745098  0.29803922 0.36078431 0.34117647 0.80784314 0.81176471 0.87058824 0.83529412 0.85882353 0.81568627 0.         0.        ]\n",
            " [0.         0.         0.         0.41568627 0.73333333 0.8745098  0.92941176 0.97254902 0.82745098 0.77647059 0.98823529 0.98039216 0.97254902 0.96078431 0.97254902 0.98823529 0.99215686 0.98039216 0.98823529 0.9372549  0.78823529 0.83137255 0.88235294 0.84313725 0.75686275 0.44313725 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.06666667 0.21176471 0.62352941 0.87058824 0.75686275 0.81568627 0.75294118 0.77254902 0.78431373 0.78431373 0.78431373 0.78431373 0.78823529 0.79607843 0.76470588 0.82352941 0.64705882 0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.18431373 0.88235294 0.75294118 0.83921569 0.79607843 0.80784314 0.8        0.8        0.80392157 0.80784314 0.8        0.83137255 0.77254902 0.85490196 0.41960784 0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00392157 0.02352941 0.         0.18039216 0.83137255 0.76470588 0.83137255 0.79215686 0.80784314 0.80392157 0.8        0.80392157 0.80784314 0.8        0.83137255 0.78431373 0.85490196 0.35686275 0.         0.01176471 0.00392157 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.04313725 0.77254902 0.78039216 0.80392157 0.79215686 0.80392157 0.80784314 0.8        0.80392157 0.81176471 0.8        0.80392157 0.80392157 0.85490196 0.30196078 0.         0.01960784 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.01176471 0.         0.00784314 0.74901961 0.77647059 0.78823529 0.80392157 0.80784314 0.80392157 0.80392157 0.80784314 0.81960784 0.80784314 0.78039216 0.81960784 0.85882353 0.29019608 0.         0.01960784 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00784314 0.         0.         0.7372549  0.77254902 0.78431373 0.81176471 0.81176471 0.8        0.81176471 0.81176471 0.82352941 0.81568627 0.77647059 0.81176471 0.86666667 0.28235294 0.         0.01568627 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00784314 0.         0.         0.84313725 0.77647059 0.79607843 0.80784314 0.81568627 0.80392157 0.81176471 0.81176471 0.82352941 0.81568627 0.78431373 0.79215686 0.87058824 0.29411765 0.         0.01568627 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.83137255 0.77647059 0.81960784 0.80784314 0.81960784 0.80784314 0.81568627 0.81176471 0.82745098 0.80784314 0.80392157 0.77647059 0.86666667 0.31372549 0.         0.01176471 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.8        0.78823529 0.80392157 0.81568627 0.81176471 0.80392157 0.82745098 0.80392157 0.82352941 0.82352941 0.81960784 0.76470588 0.86666667 0.37647059 0.         0.01176471 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.79215686 0.78823529 0.80392157 0.81960784 0.81176471 0.80392157 0.83529412 0.80784314 0.82352941 0.81960784 0.82352941 0.76078431 0.85098039 0.41176471 0.         0.00784314 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.8        0.8        0.80392157 0.81568627 0.81176471 0.80392157 0.84313725 0.81176471 0.82352941 0.81568627 0.82745098 0.75686275 0.83529412 0.45098039 0.         0.00784314 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.8        0.81176471 0.81176471 0.81568627 0.80784314 0.80784314 0.84313725 0.82352941 0.82352941 0.81176471 0.83137255 0.76470588 0.82352941 0.4627451  0.         0.00784314 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.77647059 0.81568627 0.81568627 0.81568627 0.8        0.81176471 0.83137255 0.83137255 0.82352941 0.81176471 0.82745098 0.76862745 0.81176471 0.4745098  0.         0.00392157 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.77647059 0.82352941 0.81176471 0.81568627 0.80784314 0.81960784 0.83529412 0.83137255 0.82745098 0.81176471 0.82352941 0.77254902 0.81176471 0.48627451 0.         0.00392157 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.6745098  0.82352941 0.79607843 0.78823529 0.78039216 0.8        0.81176471 0.80392157 0.8        0.78823529 0.80392157 0.77254902 0.80784314 0.49803922 0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.7372549  0.86666667 0.83921569 0.91764706 0.9254902  0.93333333 0.95686275 0.95686275 0.95686275 0.94117647 0.95294118 0.83921569 0.87843137 0.63529412 0.         0.00784314 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157 0.         0.         0.54509804 0.57254902 0.50980392 0.52941176 0.52941176 0.5372549  0.49019608 0.48627451 0.49019608 0.4745098  0.46666667 0.44705882 0.50980392 0.29803922 0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([[1.0, 3.0, 4.0, 2.0]])\n",
        "inputs = tf.convert_to_tensor(inputs)\n",
        "print(f'input to softmax function: {inputs.numpy()}')\n",
        "\n",
        "# Feed the inputs to a softmax activation function\n",
        "outputs = tf.keras.activations.softmax(inputs)\n",
        "print(f'output of softmax function: {outputs.numpy()}')\n",
        "\n",
        "# Get the sum of all values after the softmax\n",
        "sum = tf.reduce_sum(outputs)\n",
        "print(f'sum of outputs: {sum}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1vXo3gSxICK",
        "outputId": "6b1323e1-d95b-4133-c28d-7dd98151e417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input to softmax function: [[1. 3. 4. 2.]]\n",
            "output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
            "sum of outputs: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class my_call_back(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if (logs.get('accuracy') > 0.85):\n",
        "      print(\"85% Acc reached. Stopping training\\n\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "#   def on_epoch_end(self, epoch, logs={}):\n",
        "#     if(logs.get('accuracy') >= 0.6): # Experiment with changing this value\n",
        "#       print(\"\\nReached 60% accuracy so cancelling training!\")\n",
        "#       self.model.stop_training = True"
      ],
      "metadata": {
        "id": "SF0qIt7lqgbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_nn_ops import softmax\n",
        "callbacks = my_call_back()\n",
        "model = keras.Sequential([keras.layers.Flatten(), \\\n",
        "                         keras.layers.Dense(units=128, activation=tf.nn.relu), \\\n",
        "                         keras.layers.Dense(units=10, activation=tf.nn.softmax),\\\n",
        "                         ])\n",
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss=\"sparse_categorical_crossentropy\", \n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "WoB9NwcJ1Sgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx9vW1Jc56qq",
        "outputId": "ec86b0d8-8a1a-44de-d5bf-cbb54d1a8661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4973 - accuracy: 0.8265\n",
            "Epoch 2/5\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.864185% Acc reached. Stopping training\n",
            "\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3769 - accuracy: 0.8640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f344c532c10>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdXnRaw26GGU",
        "outputId": "baa3222d-e3a8-493b-b753-19d942770f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3986 - accuracy: 0.8565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.398647665977478, 0.8565000295639038]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "i3IVct0E6bnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifications[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxyndJLiirs9",
        "outputId": "2a462a45-a890-47a3-d1a9-18911dc37588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.5885072e-05 1.4776947e-07 9.8746250e-06 4.3953487e-06 2.3874578e-05 8.6870134e-02 1.6087610e-05 7.7895865e-02 2.0165544e-03 8.3306718e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiGAy7_TiuSd",
        "outputId": "6caad2a9-7f52-4f2c-f363-6a89d65df913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a5U_Nq2OjAPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}